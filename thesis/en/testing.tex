\chapter{Testing}
\label{ch:testing}

This chapter documents the comprehensive testing strategy employed throughout development. The testing infrastructure spans frontend and backend systems, employing Vitest~\cite{vitest2024} and Cypress~\cite{cypress2024} for frontend testing, pytest~\cite{pytest2024} with pytest-asyncio~\cite{pytestasyncio2024} for backend unit and integration tests, Locust~\cite{locust2024} for performance testing, and systematic fuzz testing for security validation. This multi-layered approach ensures that Qubit delivers reliable, secure, and performant quantum circuit exploration.

\section{User Interface Testing}
\label{sec:testing:ui}

User interface testing validates frontend functionality through two complementary approaches. Cypress end-to-end tests simulate complete user workflows in a real browser environment, while Vitest unit tests verify individual React components and state management in isolation.

\subsection{End-to-End Testing with Cypress}

Cypress provides automated browser testing that simulates real user interactions with the application. Unlike traditional Selenium-based testing frameworks, Cypress executes tests directly inside the browser, providing fast, reliable, and deterministic test execution. The test suite covers six major user workflows: authentication, project management, circuit composition, circuit import, partition execution, and results visualization.

The authentication test suite (\texttt{01-authentication.cy.ts}) validates the complete login flow using passwordless email authentication. The test navigates to the login page, enters a test email address, and verifies that the application displays the verification code input interface. Since automated tests cannot access external email inboxes, Cypress uses a custom command that directly retrieves verification codes from the backend's test environment:

\begin{lstlisting}[language=JavaScript, caption={Cypress authentication test validating email login flow}]
describe('Authentication', (): void => {
  it('should successfully login with valid email and code', (): void => {
    cy.emailLogin(testData.auth.testEmail)
    cy.url().should('include', '/dashboard')
    cy.get(selectors.navbar.userMenuButton).should('be.visible')
  })
})
\end{lstlisting}

The \texttt{cy.emailLogin()} custom command encapsulates the multi-step authentication process, keeping test code concise while ensuring consistency across all test suites.

Project management tests (\texttt{02-project-management.cy.ts}) verify CRUD operations for quantum circuit projects. The tests create projects with unique names, verify they appear in the project list, navigate into project details, update project metadata, duplicate existing projects, and finally delete projects. Each test ensures proper cleanup to prevent interference between test runs.

Circuit composer tests (\texttt{03-circuit-composer.cy.ts}) validate the interactive circuit design interface. These tests verify that the D3.js-based circuit canvas renders correctly, toolbar controls appear and function properly, qubits can be added and removed, and gates can be placed on the canvas. The tests interact with SVG elements directly, simulating drag-and-drop operations and verifying that gate positions update correctly in response to user actions.

Circuit import tests (\texttt{04-circuit-import.cy.ts}) ensure that users can load circuits from OpenQASM files. The test uploads a sample QASM file, verifies that the parser correctly interprets the quantum operations, and confirms that the imported circuit renders accurately on the canvas with all gates in their correct positions.

Partition execution tests (\texttt{05-circuit-partition.cy.ts}) validate the workflow for submitting circuits to SQUANDER servers. The tests configure partition parameters, initiate execution, monitor WebSocket status updates, and verify that the partition process completes successfully. Since actual SQUANDER execution can take considerable time, these tests use smaller circuits and shorter timeout values to maintain reasonable test suite runtime.

Results visualization tests (\texttt{06-circuit-results.cy.ts}) verify that partition results display correctly. The tests check that the interactive partition graph renders with correct node and edge elements, gate information appears when hovering over nodes, and users can navigate between different partition views. Figure~\ref{fig:cypress-e2e-tests} shows the Cypress test runner executing the complete test suite.

Each Cypress test follows a consistent pattern: authenticate the user, navigate to the relevant page, perform the intended action, verify the expected outcome, and clean up any created resources. The test suite employs custom commands defined in \texttt{cypress/support/commands.ts} to reduce duplication and improve readability. For example, \texttt{cy.ensureProjectAndCircuitTab(projectName)} creates a project if it doesn't exist and navigates to a circuit tab, abstracting a multi-step workflow into a single command.

Cypress tests run in both interactive mode during development and headless mode for automated testing. Interactive mode provides a graphical test runner that displays each step as it executes, enabling developers to visually debug failing tests. Headless mode runs tests in a terminal without a GUI, producing video recordings and screenshots of failures for later analysis.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{docs/screenshots/testing/e2e with cypress.png}
\caption{Cypress interactive test runner executing the complete E2E test suite in development mode, showing test specifications, assertions, and browser preview with real-time execution feedback}
\label{fig:cypress-e2e-tests}
\end{figure}

\subsection{Frontend Unit Testing with Vitest}

While Cypress validates complete user workflows, Vitest provides fast unit testing for individual React components and state management logic. Vitest is a modern testing framework designed specifically for Vite-based projects, offering near-instant test execution through intelligent module caching and parallel test execution.

The frontend test suite includes unit tests for critical application state management using Zustand stores. The authentication store test (\texttt{authStore.test.ts}) verifies that user credentials and tokens are correctly stored, retrieved, and cleared. The project store tests (\texttt{projectsStore.test.ts}) validate project state management, ensuring that projects can be added, updated, and removed from the client-side cache without corrupting state. The circuit store tests (\texttt{CircuitStore.test.ts}) verify that circuit state updates correctly when gates are added, removed, or moved, and that undo/redo operations maintain proper history.

Utility function tests validate critical business logic outside of React components. The QASM converter tests (\texttt{converter.test.ts}) verify that OpenQASM code correctly parses into the internal circuit representation and that circuits serialize back to valid QASM. The gate utility tests (\texttt{utils.test.ts}) ensure that gate validation, qubit assignment, and dependency calculation all function correctly.

Vitest integrates with the existing development workflow through npm scripts. Running \texttt{npm test} executes all unit tests in watch mode, re-running affected tests whenever source files change. This rapid feedback loop encourages developers to write tests alongside implementation code, catching bugs early in the development process.

\section{Unit and Functional Testing}
\label{sec:testing:unit}

Unit tests validate individual components in isolation by mocking external dependencies, while functional tests verify that related components interact correctly. Together, these tests provide rapid feedback and serve as executable documentation.

\subsection{Backend Unit Testing with pytest}

The backend test suite uses pytest, Python's de facto standard testing framework. pytest provides powerful fixtures for test setup, parametrized testing to reduce duplication, and clear assertion messages that simplify debugging. The backend unit tests focus on data model serialization and core business logic that operates independently of external services.

Model serialization tests (\texttt{test\_models.py}) validate the conversion between Python dataclasses and MongoDB documents. Qubit uses Pydantic models to define structured data schemas, and these tests ensure that models correctly serialize to dictionaries for database storage and deserialize back to typed objects:

\begin{lstlisting}[language=Python, caption={pytest unit test for user model serialization}]
@pytest.mark.unit
class TestUserModel:
    def test_user_to_dict(self):
        user = User(email="test@example.com", first_name="John")
        data = user.to_dict()
        assert data["email"] == "test@example.com"
        assert "created_at" in data
\end{lstlisting}

The tests also verify serialization of nested project structures containing circuits and gates. The unit tests intentionally avoid database connections or HTTP~\cite{rfc2616} requests. All external dependencies use mocks or stubs to ensure tests run quickly and deterministically. A complete unit test suite execution completes in under one second, enabling developers to run tests continuously during development without interrupting their workflow.

\subsection{Testing Business Logic and Algorithms}

Beyond data serialization, unit tests validate core algorithms that implement quantum circuit manipulation. The circuit dependency graph construction algorithm ensures that gates appear in the correct depth levels based on qubit conflicts. Unit tests verify that parallel gates on different qubits occupy the same depth level, while sequential gates on the same qubit occupy increasing depth levels.

The QASM parser implementation includes unit tests that verify correct parsing of quantum gates, qubit registers, classical registers, measurements, and comments. The tests use parameterized fixtures to validate multiple QASM snippets with a single test function, reducing test duplication and making it straightforward to add new test cases.

\subsection{Test Coverage Measurement}

The backend test suite uses \texttt{coverage.py}~\cite{coveragepy2024} to measure test coverage and identify untested code paths. Running \texttt{coverage run -m pytest} executes the test suite while tracking which lines of code execute. The coverage report highlights files with low coverage, guiding developers toward areas that need additional testing.

Current coverage targets aim for at least 80\% line coverage on core business logic, with lower coverage acceptable for configuration files and one-time initialization code. Generated HTML~\cite{html5standard} coverage reports highlight covered lines in green and uncovered lines in red, helping developers identify untested code paths and prioritize areas that need additional testing.

\section{Integration Testing}
\label{sec:testing:integration}

Integration tests validate that multiple components work correctly together, testing interactions between the API layer, database, authentication system, and external services. Unlike unit tests that mock dependencies, integration tests use real database connections and HTTP~\cite{rfc2616} clients to verify that the complete system functions as expected.

\subsection{API Endpoint Integration Testing}

The backend integration test suite validates all REST~\cite{fielding2000architectural} API endpoints using \texttt{httpx.AsyncClient} with ASGI transport. This approach executes tests in-process without starting a separate server, providing fast test execution while accurately simulating HTTP requests. The async client enables testing concurrent requests, mirroring production behavior where multiple users make simultaneous API calls. The test configuration in \texttt{conftest.py} provides pytest fixtures that handle client initialization, database setup, and automatic cleanup.

Authentication integration tests (\texttt{test\_auth.py}) validate the complete user registration and login workflow. The tests create test users, verify that duplicate registration fails, attempt login with correct and incorrect credentials, and test token refresh functionality. The tests use pytest fixtures to provide a test HTTP client and randomly generated user credentials, creating unique email addresses for each test run to prevent conflicts when tests run in parallel.

Project CRUD tests (\texttt{test\_projects.py}) validate the complete lifecycle of project resources. The tests create projects, list user projects, retrieve individual projects by ID, update project metadata, duplicate projects, and delete projects. Each operation verifies both success cases and error handling:

\begin{lstlisting}[language=Python, caption={Integration test for project CRUD operations}]
async def test_create_project(self, http_client, authenticated_user):
    token = authenticated_user
    response = await http_client.post(
        "/api/v1/projects",
        json={
            "name": "Test Project",
            "description": "Test description",
            "circuits": []
        },
        headers={"Authorization": f"Bearer {token}"}
    )
    assert response.status_code == 201
    data = response.json()
    assert data["name"] == "Test Project"
    assert "id" in data
    assert "createdAt" in data
\end{lstlisting}

The \texttt{authenticated\_user} fixture provides a valid JWT access token by registering a test user and logging in. This fixture is reused across all tests that require authentication, ensuring consistent test setup.

\subsection{Database Persistence Testing}

Integration tests verify that data persists correctly to MongoDB and that database constraints enforce data integrity. The tests use a dedicated test database that resets between test runs, ensuring that tests do not interfere with production data or with each other. The database fixture provides access to the MongoDB client for tests that need to directly verify database state or perform cleanup operations. After tests complete, the fixture deletes all test data to prevent accumulation of stale records.

\subsection{WebSocket Integration Testing}

Real-time partition status updates use WebSocket connections to stream progress information from the backend to the frontend. Integration tests validate that WebSocket connections establish correctly, that messages serialize and deserialize properly, and that connection errors handle gracefully.

The WebSocket tests (\texttt{test\_websocket.py}) use a custom \texttt{AsyncWebSocketTestHelper} class that provides both TestClient-based (ASGI transport) and real WebSocket connection support. The tests connect to the WebSocket endpoint, subscribe to a job channel, simulate partition progress updates, and verify that the client receives all expected messages. These tests ensure that the WebSocket infrastructure correctly handles connection lifecycle events, message serialization, and error conditions like network interruptions or server restarts.

\subsection{End-to-End API Workflow Testing}

Complete workflow tests validate multi-step user scenarios that span multiple API endpoints. For example, the circuit execution workflow test creates a user, authenticates, creates a project, adds a circuit, configures partition parameters, submits the circuit for execution, monitors progress through WebSocket updates, and retrieves final results.

These end-to-end tests provide confidence that all components integrate correctly and that the system can handle realistic user workflows. While slower than unit tests, they catch integration bugs that might not appear when testing components in isolation.

\section{Authentication, Performance, and Security Testing}
\label{sec:testing:auth-perf}

Beyond functional correctness, production systems must handle authentication securely, perform well under realistic load, and resist malicious inputs. This section documents the testing strategies employed to validate authentication workflows, measure performance characteristics, and identify security vulnerabilities.

\subsection{Authentication and Authorization Testing}

Authentication tests verify that only properly authenticated users can access protected resources and that authorization rules prevent users from accessing resources they do not own. The test suite includes both positive tests that verify correct authentication succeeds and negative tests that verify incorrect authentication fails.

Token validation tests ensure that invalid JWT tokens are rejected. The tests attempt to access protected endpoints with malformed tokens, expired tokens, tokens with invalid signatures, and missing tokens. These tests ensure that the authentication middleware correctly validates token format, signature, and expiration, rejecting any tokens that fail validation checks.

Cross-user access tests verify that users cannot access other users' resources even if they know the resource ID. The tests create two users, have one user create a project, and verify that the second user cannot retrieve, update, or delete that project:

\begin{lstlisting}[language=Python, caption={Cross-user resource access prevention}]
async def test_cross_user_resource_access_denied(self, http_client):
    token1 = await create_user("user1@example.com")
    token2 = await create_user("user2@example.com")

    # User 1 creates project
    project = await create_project(token1, "Private Project")

    # User 2 attempts unauthorized access
    get_response = await http_client.get(
        f"/api/v1/projects/{project['id']}",
        headers={"Authorization": f"Bearer {token2}"}
    )
    assert get_response.status_code == 404

    update_response = await http_client.put(
        f"/api/v1/projects/{project['id']}",
        json={"name": "Hacked"},
        headers={"Authorization": f"Bearer {token2}"}
    )
    assert update_response.status_code == 404
\end{lstlisting}

The test verifies that the API returns 404 Not Found rather than 403 Forbidden, preventing information disclosure about which project IDs exist in the database.

OAuth integration tests validate that Google and Microsoft authentication flows work correctly. Since these tests depend on external identity providers, we use mock OAuth responses in the test environment while providing manual testing procedures for validating production OAuth configuration.

\subsection{Performance Testing with Locust}

Performance testing measures how the system behaves under realistic and extreme load conditions. Qubit uses Locust, a modern load testing framework that simulates concurrent users through Python code. Locust tests define user behaviors as Python classes, making it straightforward to model complex user workflows.

The performance test suite defines multiple user classes that simulate different usage patterns. The \texttt{APIUser} class simulates general API usage with a mix of read and write operations. The \texttt{AuthUser} class focuses exclusively on authentication endpoints, testing registration, login, and token refresh. The \texttt{ProjectUser} class simulates CRUD operations on projects, while the \texttt{CircuitUser} class focuses on circuit composition and execution.

Each user class defines tasks with configurable weights that control how frequently each operation executes. For example, the \texttt{ProjectUser} class reads projects more frequently than it creates or deletes them, mirroring realistic usage patterns:

\begin{lstlisting}[language=Python, caption={Locust user class for project operations}]
class ProjectUser(HttpUser):
    weight = 3
    wait_time = between(1, 3)

    @task(5)
    def list_projects(self):
        self.client.get("/api/v1/projects",
            headers={"Authorization": f"Bearer {self.token}"})

    @task(2)
    def create_project(self):
        self.client.post("/api/v1/projects",
            json={"name": "Test", "circuits": []},
            headers={"Authorization": f"Bearer {self.token}"})
\end{lstlisting}

The \texttt{wait\_time = between(1, 3)} declaration configures each simulated user to wait 1-3 seconds between requests, modeling realistic user behavior rather than hammering the server with continuous requests.

Locust provides both a web UI for interactive load testing and a headless mode for automated testing (Figure~\ref{fig:locust-performance-testing}). The web UI displays real-time statistics including request rate, response times (p50, p95, p99), error rate, and total request counts. Developers can adjust the number of simulated users and spawn rate while the test runs, enabling exploration of how the system behaves as load increases.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{docs/screenshots/testing/performance with locust.png}
\caption{Locust web interface displaying real-time performance metrics with request statistics, response time percentiles (p50, p95, p99), failure rates, and throughput charts during load testing}
\label{fig:locust-performance-testing}
\end{figure}

Performance tests establish baseline metrics for acceptable system behavior:

\begin{itemize}
    \item Response time (p95) should remain below 500ms for API requests
    \item Response time (p99) should remain below 1s for API requests
    \item Error rate should stay below 1\% under normal load
    \item System should support at least 50 concurrent users
\end{itemize}

Running performance tests before and after significant changes helps identify performance regressions early. For example, adding database indexes reduced project listing response time from 800ms to 150ms under load, a improvement discovered through Locust testing.

The stress test configuration (\texttt{locust\_stress\_test.py}) gradually increases load until the system exhibits unacceptable error rates or response times. This identifies the system's breaking point and reveals bottlenecks that would prevent scaling to larger user populations. During stress testing, monitoring tools track CPU usage, memory consumption, database connection pool utilization, and other system metrics to identify resource constraints.

\subsection{Security and Fuzz Testing}

Fuzz testing validates that the application handles malformed and malicious inputs gracefully without crashing or exposing security vulnerabilities. The fuzz test suite (\texttt{test\_fuzz.py}) systematically tests API endpoints with extreme and invalid inputs.

Email validation fuzz tests submit various malformed email addresses to the registration endpoint, including empty strings, extremely long inputs, missing components (no @ symbol, no domain, no local part), and inputs containing newlines or null bytes. The tests verify that the server returns appropriate error codes (400 or 422) rather than crashing with 500 Internal Server Error, ensuring that input validation catches malformed data before it reaches backend processing logic.

XSS prevention tests verify that user-supplied text does not execute as JavaScript~\cite{ecmascript2025}. The tests submit project names containing script tags and verify that the API either rejects the input or properly escapes it when returning data:

\begin{lstlisting}[language=Python, caption={XSS attack prevention testing}]
async def test_fuzz_project_name(self, http_client, authenticated_user):
    fuzz_inputs = [
        "test<script>alert('xss')</script>",
        "<img src=x onerror=alert('xss')>",
        "test';DROP TABLE projects;--"
    ]
    for name in fuzz_inputs:
        response = await http_client.post(
            "/api/v1/projects",
            json={"name": name, "circuits": []},
            headers={"Authorization": f"Bearer {authenticated_user}"}
        )
        assert response.status_code != 500
        if response.status_code == 201:
            data = response.json()
            # Verify data is properly escaped in response
            assert data["name"] == name  # Stored as-is
            # XSS prevention happens during frontend rendering
\end{lstlisting}

The backend stores user input as-is; React components handle HTML~\cite{html5standard} escaping to prevent XSS, following OWASP security best practices~\cite{owasp2021top10}.

NoSQL injection tests verify that MongoDB queries properly sanitize user input. While MongoDB is less vulnerable to injection than SQL databases, improper query construction can still enable NoSQL injection attacks. The tests submit malicious payloads containing MongoDB query operators like \texttt{\$gt}, \texttt{\$ne}, and \texttt{\$where}, ensuring that query parameters undergo proper validation and sanitization before constructing database queries, preventing attackers from injecting MongoDB operators that could bypass authorization checks.

Malformed JSON~\cite{rfc7159} tests verify that the API handles invalid request bodies gracefully, including invalid JSON syntax, trailing commas, missing braces, unquoted strings, and null values. These tests ensure that the FastAPI framework's automatic JSON parsing and validation catches malformed input before it reaches application code.

Authentication token fuzz tests verify that the JWT~\cite{rfc7519} validation logic handles malformed tokens safely, including empty tokens, extremely long tokens, tokens with null bytes, and tokens with invalid signatures. The tests verify that invalid tokens result in authentication errors rather than server crashes, ensuring that the system degrades gracefully when facing malformed authentication attempts.

Together, these fuzz tests provide confidence that Qubit handles unexpected and malicious inputs safely. While no testing can guarantee complete security, systematic fuzz testing significantly reduces the attack surface by validating that input validation and sanitization work correctly across all API endpoints.